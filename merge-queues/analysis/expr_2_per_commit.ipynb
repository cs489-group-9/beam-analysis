{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from google.cloud import bigquery\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_commits_for_commit_build_times(\n",
    "    commits: pd.DataFrame,\n",
    "    workflow_runs: pd.DataFrame,\n",
    "    batch_max_wait_time: int,\n",
    "    observations: List,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of commits, batch them into groups based on the time between commits\n",
    "    \"\"\"\n",
    "    current_batch_workflows = defaultdict(list)\n",
    "    current_batch_end_time = commits.iloc[0][\"date\"] + pd.Timedelta(\n",
    "        minutes=batch_max_wait_time\n",
    "    )\n",
    "    curr_commit_batch = [commits.iloc[0]]\n",
    "    curr_batch_build_time = 0\n",
    "\n",
    "    for i in range(1, len(commits)):\n",
    "        curr_commit = commits.iloc[i]\n",
    "\n",
    "        if curr_commit[\"date\"] > current_batch_end_time:\n",
    "            for _, build_times in current_batch_workflows.items():\n",
    "                curr_batch_build_time += np.mean(\n",
    "                    build_times\n",
    "                )  # Take the average of all builds for a certain workflow across the batch of commits\n",
    "\n",
    "            current_batch_end_time = curr_commit[\"date\"] + pd.Timedelta(\n",
    "                minutes=batch_max_wait_time\n",
    "            )\n",
    "\n",
    "            for c in curr_commit_batch:\n",
    "                observations.append({\"commit\": c[\"sha\"], \"build_minutes\": curr_batch_build_time / len(curr_commit_batch), \"delay\": batch_max_wait_time})\n",
    "\n",
    "            current_batch_workflows = defaultdict(list)\n",
    "            curr_commit_batch = []\n",
    "            curr_batch_build_time = 0\n",
    "\n",
    "        curr_commit_batch.append(curr_commit)\n",
    "        workflows_for_commit = workflow_runs.loc[\n",
    "            workflow_runs[\"head_sha\"] == curr_commit[\"sha\"]\n",
    "        ]\n",
    "        for _, workflow in workflows_for_commit.iterrows():\n",
    "            current_batch_workflows[workflow[\"workflow_id\"]].append(\n",
    "                workflow[\"build_minutes\"]\n",
    "            )\n",
    "\n",
    "    # Process the last batch\n",
    "    for _, build_times in current_batch_workflows.items():\n",
    "        curr_batch_build_time += np.mean(\n",
    "            build_times\n",
    "        )  # Take the average of all builds for a certain workflow across the batch of commits\n",
    "\n",
    "    for c in curr_commit_batch:\n",
    "        observations.append({\"commit\": c[\"sha\"], \"build_minutes\": curr_batch_build_time / len(curr_commit_batch), \"delay\": batch_max_wait_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_control(commits_df: pd.DataFrame, workflow_runs: pd.DataFrame):\n",
    "    control_data = {commits_df[\"sha\"]: 0 for _, commits_df in commits_df.iterrows()}\n",
    "    for _, workflow in workflow_runs.iterrows():\n",
    "        control_data[workflow[\"head_sha\"]] += workflow[\"build_minutes\"]\n",
    "    control_df = pd.DataFrame(list(control_data.items()), columns=[\"commit\", \"build_minutes\"])\n",
    "    control_df[\"delay\"] = 0\n",
    "    return control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monte_carlo_simulation(\n",
    "    all_commits: pd.DataFrame, workflow_runs: pd.DataFrame, iterations: int = 10\n",
    "):\n",
    "    observations = []\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        bootstrap_sample = all_commits.sample(\n",
    "            n=1000, replace=True\n",
    "        ).sort_index()  # retain original sorted order which which started at the earliest commit in range and is ascending by time\n",
    "\n",
    "        merge_queue_batch_delay = np.random.randint(1, 61)\n",
    "        batch_commits_for_commit_build_times(\n",
    "            bootstrap_sample, workflow_runs, merge_queue_batch_delay, observations\n",
    "        )\n",
    "    return pd.DataFrame(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_commit_shas = \"SELECT sha, commit.committer.date FROM `scientific-glow-417622.beam.commits` ORDER BY commit.committer.date ASC\"\n",
    "query_for_workflow_runs = \"\"\"\n",
    "    SELECT\n",
    "        workflow_run.head_sha,\n",
    "        workflow_run.name,\n",
    "        workflow_run.workflow_id,\n",
    "        workflow_run.run_started_at,\n",
    "        workflow_run.created_at,\n",
    "        workflow_run.updated_at,\n",
    "        TIMESTAMP_DIFF(workflow_run.updated_at, workflow_run.created_at, SECOND) / 60.0 AS build_minutes\n",
    "    FROM\n",
    "        `scientific-glow-417622.beam.commits` AS commits\n",
    "    CROSS JOIN\n",
    "        `scientific-glow-417622.beam.push_and_schedule_workflows`,\n",
    "        UNNEST(workflow_runs) AS workflow_run\n",
    "    WHERE\n",
    "        commits.sha = workflow_run.head_sha AND workflow_run.event = 'push'\n",
    "        \"\"\"\n",
    "logger.info(\"Fetching data from BigQuery...\")\n",
    "commits_df = client.query_and_wait(query_for_commit_shas).to_dataframe()\n",
    "workflow_runs_df = client.query_and_wait(query_for_workflow_runs).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calculating control Build minutes...\")\n",
    "control_df = process_control(commits_df, workflow_runs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Running Monte Carlo Simulation...\")\n",
    "observations_df = run_monte_carlo_simulation(\n",
    "    commits_df, workflow_runs_df, iterations=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for all rows matching commit in observations_df dataframe\n",
    "filtered_df = observations_df[observations_df['commit'] == \"7ca1a5ddffdc777a6831d2d4c1d3b6dcd9905219\"]\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation: Data Aggregation step was assisted by ChatGPT\n",
    "aggregated_observations_df = observations_df.groupby(\"commit\").agg(\n",
    "    build_minutes_mean=(\"build_minutes\", 'mean'),\n",
    "    delay_mean=(\"delay\", 'mean'),\n",
    "    build_minutes_2_5_percentile=(\"build_minutes\", lambda x: np.percentile(x, 2.5)),\n",
    "    build_minutes_97_5_percentile=(\"build_minutes\", lambda x: np.percentile(x, 97.5)),\n",
    "    delay_2_5_percentile=(\"delay\", lambda x: np.percentile(x, 2.5)),\n",
    "    delay_97_5_percentile=(\"delay\", lambda x: np.percentile(x, 97.5)),\n",
    ").reset_index()\n",
    "aggregated_observations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outlier\n",
    "# control_df = control_df[\n",
    "#     control_df[\"build_minutes\"] < 3000\n",
    "# ]\n",
    "\n",
    "plt.scatter(control_df[\"delay\"], control_df[\"build_minutes\"],  label=\"Control\")\n",
    "plt.title(\"Mean Delay vs. Mean Build Minutes for Each Commit (Control)\")\n",
    "plt.xlabel(\"Mean Delay\")\n",
    "plt.ylabel(\"Mean Build Minutes\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outlier\n",
    "aggregated_observations_df[aggregated_observations_df[\"build_minutes_mean\"] > 8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers (anything below 600 and anything equal to 0)\n",
    "# Commits with build minutes equal to 0 likely made very minimal code changes or failed right before they began\n",
    "# Commits with build minutes above 1000 are outliers because they likely represent commits that spent a very long time running or caused a hung CI job.\n",
    "aggregated_observations_df = aggregated_observations_df[aggregated_observations_df[\"build_minutes_mean\"] < 5000]\n",
    "control_df = control_df[control_df[\"build_minutes\"] < 5000]\n",
    "\n",
    "plt.scatter(aggregated_observations_df[\"delay_mean\"], aggregated_observations_df[\"build_minutes_mean\"])\n",
    "plt.title('Mean Build Minutes vs. Mean Delay for Each Commit (Monte Carlo Simulation)')\n",
    "plt.ylabel('Mean Build Minutes')\n",
    "plt.xlabel('Mean Delay')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical test (One Tailed T Test) without error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = aggregated_observations_df.merge(control_df, on=\"commit\", how=\"inner\")\n",
    "\n",
    "merge_queue_build_minutes = merged_df[\"build_minutes_mean\"]\n",
    "control_build_minutes = merged_df[\"build_minutes\"]\n",
    "\n",
    "print(np.mean(merge_queue_build_minutes), np.mean(control_build_minutes))\n",
    "\n",
    "# We just want to know if the merge queue build minutes are less than the control build minutes\n",
    "statistic, p_value = stats.ttest_rel(merge_queue_build_minutes, control_build_minutes, alternative='less')\n",
    "statistic, p_value\n",
    "# merged_df.to_csv(\"merged_df.csv\", index=False)"
   ]
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlay Control on Monte-Carlo Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(aggregated_observations_df[\"delay_mean\"], aggregated_observations_df[\"build_minutes_mean\"], label=\"Merge Queue\")\n",
    "plt.scatter(control_df[\"delay\"], control_df[\"build_minutes\"], label=\"Control\")\n",
    "plt.title('Mean Build Minutes vs. Mean Delay for Each Commit (Monte Carlo Simulation)')\n",
    "plt.ylabel('Mean Build Minutes')\n",
    "plt.xlabel('Mean Delay')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
